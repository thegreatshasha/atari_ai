Initialize replay memory D to capacity N
Initialize action-value function Q with random weights

for episode = 1, M do

Initialise sequence s1 = {x1} and preprocessed sequenced Ï†1 = Ï†(s1)
    for t = 1, T do
        With probability  select a random action at
        otherwise select at = maxa Qâˆ—(Ï†(st), a; Î¸)
        
        Execute action at in emulator and observe reward rt and image xt+1
        Set st+1 = st, at, xt+1 and preprocess Ï†t+1 = Ï†(st+1)
        
        Store transition (Ï†t, at, rt, Ï†t+1) in D
        
        Sample random minibatch of transitions (Ï†j , aj , rj , Ï†j+1) from D
        
        Set yj =rj for terminal Ï†j+1 rj + Î³ 
        maxa0 Q(Ï†j+1, a0; Î¸) for non-terminal Ï†j+1
        
        Perform a gradient descent step on (yj âˆ’ Q(Ï†j , aj ; Î¸))2 according to equation 3
    end for
end for

https://github.com/spragunr/deep_q_rl/blob/d25cb151bfb1d1b0e9302e5483ceef2cd6245e99/deep_q_rl/ale_data_set.py#L79 for understanding phi